{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5026b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0fe259",
   "metadata": {},
   "source": [
    "# Self Attention to get h_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c50a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HC_READTEXT_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only\"   # folder with *_tokens_for_selfattn.npz\n",
    "HC_READTEXT_OUT_DIR      = os.path.join(HC_READTEXT_NPZ_DIR, \"..\", \"hc_ReadText_selfattn\")\n",
    "PATTERN      = \"*_tokens_for_selfattn.npz\"\n",
    "\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_N    = 64             # sentences per forward pass; lower if OOM\n",
    "D_MODEL    = 768\n",
    "N_HEADS    = 8\n",
    "N_LAYERS   = 2\n",
    "D_FF       = 2048\n",
    "DROPOUT    = 0.1\n",
    "SAVE_DTYPE = np.float32     # set to np.float16 to save disk\n",
    "SAVE_POOLED = True          # also save pooled [N, D_MODEL]\n",
    "\n",
    "os.makedirs(HC_READTEXT_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb50a2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 NPZ files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roshidatdnslab/anaconda3/envs/sheedah/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID00_hc_0_0_0_h_text_selfattn.npz | h_text (15, 512, 768), pooled (15, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID01_hc_0_0_0_h_text_selfattn.npz | h_text (24, 512, 768), pooled (24, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID03_hc_0_0_0_h_text_selfattn.npz | h_text (30, 512, 768), pooled (30, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID05_hc_0_0_0_h_text_selfattn.npz | h_text (20, 512, 768), pooled (20, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID08_hc_0_0_0_h_text_selfattn.npz | h_text (34, 512, 768), pooled (34, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID09_hc_0_0_0_h_text_selfattn.npz | h_text (24, 512, 768), pooled (24, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID10_hc_0_0_0_h_text_selfattn.npz | h_text (15, 512, 768), pooled (15, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID11_hc_0_0_0_h_text_selfattn.npz | h_text (45, 512, 768), pooled (45, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID12_hc_0_0_0_h_text_selfattn.npz | h_text (18, 512, 768), pooled (18, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID14_hc_0_0_0_h_text_selfattn.npz | h_text (22, 512, 768), pooled (22, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID15_hc_0_0_0_h_text_selfattn.npz | h_text (19, 512, 768), pooled (19, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID19_hc_0_0_0_h_text_selfattn.npz | h_text (24, 512, 768), pooled (24, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID21_hc_0_0_0_h_text_selfattn.npz | h_text (22, 512, 768), pooled (22, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID22_hc_0_0_0_h_text_selfattn.npz | h_text (18, 512, 768), pooled (18, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID23_hc_0_0_0_h_text_selfattn.npz | h_text (29, 512, 768), pooled (29, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID25_hc_0_0_0_h_text_selfattn.npz | h_text (19, 512, 768), pooled (19, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID26_hc_0_0_0_h_text_selfattn.npz | h_text (23, 512, 768), pooled (23, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID28_hc_0_0_0_h_text_selfattn.npz | h_text (28, 512, 768), pooled (28, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID31_hc_0_1_1_h_text_selfattn.npz | h_text (30, 512, 768), pooled (30, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID35_hc_0_0_0_h_text_selfattn.npz | h_text (17, 512, 768), pooled (17, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_ReadText_berts_feats_tokens_only/../hc_ReadText_selfattn/ID36_hc_0_0_0_h_text_selfattn.npz | h_text (11, 512, 768), pooled (11, 768)\n"
     ]
    }
   ],
   "source": [
    "# ================= Self-Attention Encoder =================\n",
    "class SelfAttentionTextEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # token_embeddings: [B, L, d_in], attention_mask: [B, L] (True/1=valid)\n",
    "        x = self.in_proj(token_embeddings)\n",
    "        key_padding_mask = ~attention_mask.bool()              # True = ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x  # [B, L, d_model]\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)\n",
    "\n",
    "# ===================== Main Loop =====================\n",
    "def main():\n",
    "    paths = sorted(glob.glob(os.path.join(HC_READTEXT_NPZ_DIR, PATTERN)))\n",
    "    print(f\"Found {len(paths)} NPZ files.\")\n",
    "    if not paths: return\n",
    "\n",
    "    encoder = None  # build after seeing d_in\n",
    "\n",
    "    for tpath in paths:\n",
    "        base = os.path.basename(tpath).replace(\"_tokens_for_selfattn.npz\", \"\")\n",
    "        with np.load(tpath, allow_pickle=True) as arr:\n",
    "            tokens = torch.from_numpy(arr[\"token_embeddings\"]).float()  # [N, L, d_in]\n",
    "            mask   = torch.from_numpy(arr[\"attention_mask\"]).bool()     # [N, L]\n",
    "\n",
    "        N, L, d_in = tokens.shape\n",
    "\n",
    "        # Build encoder once with correct input dim\n",
    "        if encoder is None:\n",
    "            encoder = SelfAttentionTextEncoder(\n",
    "                d_in=d_in, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "\n",
    "        # Compute h_text in chunks\n",
    "        h_chunks, pool_chunks = [], []\n",
    "        for i in range(0, N, BATCH_N):\n",
    "            x = tokens[i:i+BATCH_N].to(DEVICE)\n",
    "            m = mask[i:i+BATCH_N].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                h = encoder(x, m)                    # [b, L, D_MODEL]\n",
    "            h_chunks.append(h.cpu())\n",
    "            if SAVE_POOLED:\n",
    "                pool_chunks.append(masked_mean(h, m).cpu())\n",
    "\n",
    "        h_text = torch.cat(h_chunks, dim=0).numpy().astype(SAVE_DTYPE)       # [N, L, D_MODEL]\n",
    "        out_mask = mask.numpy()                                               # [N, L]\n",
    "        if SAVE_POOLED:\n",
    "            h_text_pooled = torch.cat(pool_chunks, dim=0).numpy().astype(SAVE_DTYPE)  # [N, D_MODEL]\n",
    "\n",
    "        out_path = os.path.join(HC_READTEXT_OUT_DIR, f\"{base}_h_text_selfattn.npz\")\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_text=h_text,\n",
    "            attention_mask=out_mask,                 # keep the same mask\n",
    "            h_text_pooled=(h_text_pooled if SAVE_POOLED else None),\n",
    "            d_model=np.array(D_MODEL),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_text_npz=os.path.basename(tpath),\n",
    "        )\n",
    "        print(f\"Saved: {out_path} | h_text {h_text.shape}\" + (f\", pooled {h_text_pooled.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2427848",
   "metadata": {},
   "outputs": [],
   "source": [
    "PD_TEXT_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only\"   # folder with *_tokens_for_selfattn.npz\n",
    "PD_OUT_DIR      = os.path.join(PD_TEXT_NPZ_DIR, \"..\", \"PD_ReadText_selfattn\")\n",
    "PATTERN      = \"*_tokens_for_selfattn.npz\"\n",
    "\n",
    "os.makedirs(PD_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e367bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 NPZ files.\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID02_pd_2_0_0_h_text_selfattn.npz | h_text (23, 512, 768), pooled (23, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID04_pd_2_0_1_h_text_selfattn.npz | h_text (16, 512, 768), pooled (16, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID06_pd_3_1_1_h_text_selfattn.npz | h_text (23, 512, 768), pooled (23, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID07_pd_2_0_0_h_text_selfattn.npz | h_text (11, 512, 768), pooled (11, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID13_pd_3_2_2_h_text_selfattn.npz | h_text (14, 512, 768), pooled (14, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID16_pd_2_0_0_h_text_selfattn.npz | h_text (29, 512, 768), pooled (29, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID17_pd_2_1_0_h_text_selfattn.npz | h_text (3, 512, 768), pooled (3, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID18_pd_4_3_3_h_text_selfattn.npz | h_text (7, 512, 768), pooled (7, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID20_pd_3_0_1_h_text_selfattn.npz | h_text (17, 512, 768), pooled (17, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID24_pd_2_0_0_h_text_selfattn.npz | h_text (24, 512, 768), pooled (24, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID27_pd_4_1_1_h_text_selfattn.npz | h_text (11, 512, 768), pooled (11, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID29_pd_3_1_2_h_text_selfattn.npz | h_text (13, 512, 768), pooled (13, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID30_pd_2_1_1_h_text_selfattn.npz | h_text (16, 512, 768), pooled (16, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID32_pd_3_1_1_h_text_selfattn.npz | h_text (12, 512, 768), pooled (12, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID33_pd_3_2_2_h_text_selfattn.npz | h_text (25, 512, 768), pooled (25, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_ReadText_berts_feats_tokens_only/../PD_ReadText_selfattn/ID34_pd_2_0_0_h_text_selfattn.npz | h_text (22, 512, 768), pooled (22, 768)\n"
     ]
    }
   ],
   "source": [
    "# ================= Self-Attention Encoder =================\n",
    "class SelfAttentionTextEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # token_embeddings: [B, L, d_in], attention_mask: [B, L] (True/1=valid)\n",
    "        x = self.in_proj(token_embeddings)\n",
    "        key_padding_mask = ~attention_mask.bool()              # True = ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x  # [B, L, d_model]\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)\n",
    "\n",
    "# ===================== Main Loop =====================\n",
    "def main():\n",
    "    paths = sorted(glob.glob(os.path.join(PD_TEXT_NPZ_DIR, PATTERN)))\n",
    "    print(f\"Found {len(paths)} NPZ files.\")\n",
    "    if not paths: return\n",
    "\n",
    "    encoder = None  # build after seeing d_in\n",
    "\n",
    "    for tpath in paths:\n",
    "        base = os.path.basename(tpath).replace(\"_tokens_for_selfattn.npz\", \"\")\n",
    "        with np.load(tpath, allow_pickle=True) as arr:\n",
    "            tokens = torch.from_numpy(arr[\"token_embeddings\"]).float()  # [N, L, d_in]\n",
    "            mask   = torch.from_numpy(arr[\"attention_mask\"]).bool()     # [N, L]\n",
    "\n",
    "        N, L, d_in = tokens.shape\n",
    "\n",
    "        # Build encoder once with correct input dim\n",
    "        if encoder is None:\n",
    "            encoder = SelfAttentionTextEncoder(\n",
    "                d_in=d_in, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "\n",
    "        # Compute h_text in chunks\n",
    "        h_chunks, pool_chunks = [], []\n",
    "        for i in range(0, N, BATCH_N):\n",
    "            x = tokens[i:i+BATCH_N].to(DEVICE)\n",
    "            m = mask[i:i+BATCH_N].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                h = encoder(x, m)                    # [b, L, D_MODEL]\n",
    "            h_chunks.append(h.cpu())\n",
    "            if SAVE_POOLED:\n",
    "                pool_chunks.append(masked_mean(h, m).cpu())\n",
    "\n",
    "        h_text = torch.cat(h_chunks, dim=0).numpy().astype(SAVE_DTYPE)       # [N, L, D_MODEL]\n",
    "        out_mask = mask.numpy()                                               # [N, L]\n",
    "        if SAVE_POOLED:\n",
    "            h_text_pooled = torch.cat(pool_chunks, dim=0).numpy().astype(SAVE_DTYPE)  # [N, D_MODEL]\n",
    "\n",
    "        out_path = os.path.join(PD_OUT_DIR, f\"{base}_h_text_selfattn.npz\")\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_text=h_text,\n",
    "            attention_mask=out_mask,                 # keep the same mask\n",
    "            h_text_pooled=(h_text_pooled if SAVE_POOLED else None),\n",
    "            d_model=np.array(D_MODEL),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_text_npz=os.path.basename(tpath),\n",
    "        )\n",
    "        print(f\"Saved: {out_path} | h_text {h_text.shape}\" + (f\", pooled {h_text_pooled.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d621bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "HC_Spontaneous_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only\"   # folder with *_tokens_for_selfattn.npz\n",
    "HC_Spontaneous_OUT_DIR      = os.path.join(HC_Spontaneous_NPZ_DIR, \"..\", \"HC_Spontaneous_selfattn\")\n",
    "PATTERN      = \"*_tokens_for_selfattn.npz\"\n",
    "\n",
    "os.makedirs(HC_Spontaneous_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd2fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 NPZ files.\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID00_hc_0_0_0_h_text_selfattn.npz | h_text (58, 512, 768), pooled (58, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID01_hc_0_0_0_h_text_selfattn.npz | h_text (27, 512, 768), pooled (27, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID03_hc_0_0_0_h_text_selfattn.npz | h_text (33, 512, 768), pooled (33, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID05_hc_0_0_0_h_text_selfattn.npz | h_text (35, 512, 768), pooled (35, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID08_hc_0_0_0_h_text_selfattn.npz | h_text (35, 512, 768), pooled (35, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID09_hc_0_0_0_h_text_selfattn.npz | h_text (36, 512, 768), pooled (36, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID10_hc_0_0_0_h_text_selfattn.npz | h_text (66, 512, 768), pooled (66, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID11_hc_0_0_0_h_text_selfattn.npz | h_text (45, 512, 768), pooled (45, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID12_hc_0_0_0_h_text_selfattn.npz | h_text (32, 512, 768), pooled (32, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID14_hc_0_0_0_h_text_selfattn.npz | h_text (53, 512, 768), pooled (53, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID15_hc_0_0_0_h_text_selfattn.npz | h_text (40, 512, 768), pooled (40, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID19_hc_0_0_0_h_text_selfattn.npz | h_text (35, 512, 768), pooled (35, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID21_hc_0_0_0_h_text_selfattn.npz | h_text (12, 512, 768), pooled (12, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID22hc_0_0_0_h_text_selfattn.npz | h_text (49, 512, 768), pooled (49, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID23_hc_0_0_0_h_text_selfattn.npz | h_text (19, 512, 768), pooled (19, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID25_hc_0_0_0_h_text_selfattn.npz | h_text (32, 512, 768), pooled (32, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID26_hc_0_0_0_h_text_selfattn.npz | h_text (45, 512, 768), pooled (45, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID28_hc_0_0_0_h_text_selfattn.npz | h_text (31, 512, 768), pooled (31, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID31_hc_0_1_1_h_text_selfattn.npz | h_text (21, 512, 768), pooled (21, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID35_hc_0_0_0_h_text_selfattn.npz | h_text (27, 512, 768), pooled (27, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/HC_Spontaneous_berts_feats_tokens_only/../HC_Spontaneous_selfattn/ID36_hc_0_0_0_h_text_selfattn.npz | h_text (40, 512, 768), pooled (40, 768)\n"
     ]
    }
   ],
   "source": [
    "# ================= Self-Attention Encoder =================\n",
    "class SelfAttentionTextEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # token_embeddings: [B, L, d_in], attention_mask: [B, L] (True/1=valid)\n",
    "        x = self.in_proj(token_embeddings)\n",
    "        key_padding_mask = ~attention_mask.bool()              # True = ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x  # [B, L, d_model]\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)\n",
    "\n",
    "# ===================== Main Loop =====================\n",
    "def main():\n",
    "    paths = sorted(glob.glob(os.path.join(HC_Spontaneous_NPZ_DIR, PATTERN)))\n",
    "    print(f\"Found {len(paths)} NPZ files.\")\n",
    "    if not paths: return\n",
    "\n",
    "    encoder = None  # build after seeing d_in\n",
    "\n",
    "    for tpath in paths:\n",
    "        base = os.path.basename(tpath).replace(\"_tokens_for_selfattn.npz\", \"\")\n",
    "        with np.load(tpath, allow_pickle=True) as arr:\n",
    "            tokens = torch.from_numpy(arr[\"token_embeddings\"]).float()  # [N, L, d_in]\n",
    "            mask   = torch.from_numpy(arr[\"attention_mask\"]).bool()     # [N, L]\n",
    "\n",
    "        N, L, d_in = tokens.shape\n",
    "\n",
    "        # Build encoder once with correct input dim\n",
    "        if encoder is None:\n",
    "            encoder = SelfAttentionTextEncoder(\n",
    "                d_in=d_in, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "\n",
    "        # Compute h_text in chunks\n",
    "        h_chunks, pool_chunks = [], []\n",
    "        for i in range(0, N, BATCH_N):\n",
    "            x = tokens[i:i+BATCH_N].to(DEVICE)\n",
    "            m = mask[i:i+BATCH_N].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                h = encoder(x, m)                    # [b, L, D_MODEL]\n",
    "            h_chunks.append(h.cpu())\n",
    "            if SAVE_POOLED:\n",
    "                pool_chunks.append(masked_mean(h, m).cpu())\n",
    "\n",
    "        h_text = torch.cat(h_chunks, dim=0).numpy().astype(SAVE_DTYPE)       # [N, L, D_MODEL]\n",
    "        out_mask = mask.numpy()                                               # [N, L]\n",
    "        if SAVE_POOLED:\n",
    "            h_text_pooled = torch.cat(pool_chunks, dim=0).numpy().astype(SAVE_DTYPE)  # [N, D_MODEL]\n",
    "\n",
    "        out_path = os.path.join(HC_Spontaneous_OUT_DIR, f\"{base}_h_text_selfattn.npz\")\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_text=h_text,\n",
    "            attention_mask=out_mask,                 # keep the same mask\n",
    "            h_text_pooled=(h_text_pooled if SAVE_POOLED else None),\n",
    "            d_model=np.array(D_MODEL),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_text_npz=os.path.basename(tpath),\n",
    "        )\n",
    "        print(f\"Saved: {out_path} | h_text {h_text.shape}\" + (f\", pooled {h_text_pooled.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f78a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PD_Spontaneous_TEXT_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only\"   # folder with *_tokens_for_selfattn.npz\n",
    "PD_Spontaneous_OUT_DIR      = os.path.join(PD_Spontaneous_TEXT_NPZ_DIR, \"..\", \"PD_Spontaneous_selfattn\")\n",
    "PATTERN      = \"*_tokens_for_selfattn.npz\"\n",
    "\n",
    "os.makedirs(PD_Spontaneous_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6705c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 NPZ files.\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID02_pd_2_0_0_h_text_selfattn.npz | h_text (32, 512, 768), pooled (32, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID04_pd_2_0_1_h_text_selfattn.npz | h_text (35, 512, 768), pooled (35, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID06_pd_3_1_1_h_text_selfattn.npz | h_text (23, 512, 768), pooled (23, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID07_pd_2_0_0_h_text_selfattn.npz | h_text (40, 512, 768), pooled (40, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID13_pd_3_2_2_h_text_selfattn.npz | h_text (36, 512, 768), pooled (36, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID16_pd_2_0_0_h_text_selfattn.npz | h_text (38, 512, 768), pooled (38, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID17_pd_2_1_0_h_text_selfattn.npz | h_text (44, 512, 768), pooled (44, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID20_pd_3_0_1_h_text_selfattn.npz | h_text (18, 512, 768), pooled (18, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID24_pd_2_0_0_h_text_selfattn.npz | h_text (30, 512, 768), pooled (30, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID27_pd_4_1_1_h_text_selfattn.npz | h_text (41, 512, 768), pooled (41, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID29_pd_3_1_2_h_text_selfattn.npz | h_text (29, 512, 768), pooled (29, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID30_pd_2_1_1_h_text_selfattn.npz | h_text (19, 512, 768), pooled (19, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID32_pd_3_1_1_h_text_selfattn.npz | h_text (5, 512, 768), pooled (5, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID33_pd_3_2_2_h_text_selfattn.npz | h_text (14, 512, 768), pooled (14, 768)\n",
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/berts_text_embeddings/PD_Spontaneous_berts_feats_tokens_only/../PD_Spontaneous_selfattn/ID34_pd_2_0_0_h_text_selfattn.npz | h_text (17, 512, 768), pooled (17, 768)\n"
     ]
    }
   ],
   "source": [
    "# ================= Self-Attention Encoder =================\n",
    "class SelfAttentionTextEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # token_embeddings: [B, L, d_in], attention_mask: [B, L] (True/1=valid)\n",
    "        x = self.in_proj(token_embeddings)\n",
    "        key_padding_mask = ~attention_mask.bool()              # True = ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x  # [B, L, d_model]\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)\n",
    "\n",
    "# ===================== Main Loop =====================\n",
    "def main():\n",
    "    paths = sorted(glob.glob(os.path.join(PD_Spontaneous_TEXT_NPZ_DIR, PATTERN)))\n",
    "    print(f\"Found {len(paths)} NPZ files.\")\n",
    "    if not paths: return\n",
    "\n",
    "    encoder = None  # build after seeing d_in\n",
    "\n",
    "    for tpath in paths:\n",
    "        base = os.path.basename(tpath).replace(\"_tokens_for_selfattn.npz\", \"\")\n",
    "        with np.load(tpath, allow_pickle=True) as arr:\n",
    "            tokens = torch.from_numpy(arr[\"token_embeddings\"]).float()  # [N, L, d_in]\n",
    "            mask   = torch.from_numpy(arr[\"attention_mask\"]).bool()     # [N, L]\n",
    "\n",
    "        N, L, d_in = tokens.shape\n",
    "\n",
    "        # Build encoder once with correct input dim\n",
    "        if encoder is None:\n",
    "            encoder = SelfAttentionTextEncoder(\n",
    "                d_in=d_in, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "\n",
    "        # Compute h_text in chunks\n",
    "        h_chunks, pool_chunks = [], []\n",
    "        for i in range(0, N, BATCH_N):\n",
    "            x = tokens[i:i+BATCH_N].to(DEVICE)\n",
    "            m = mask[i:i+BATCH_N].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                h = encoder(x, m)                    # [b, L, D_MODEL]\n",
    "            h_chunks.append(h.cpu())\n",
    "            if SAVE_POOLED:\n",
    "                pool_chunks.append(masked_mean(h, m).cpu())\n",
    "\n",
    "        h_text = torch.cat(h_chunks, dim=0).numpy().astype(SAVE_DTYPE)       # [N, L, D_MODEL]\n",
    "        out_mask = mask.numpy()                                               # [N, L]\n",
    "        if SAVE_POOLED:\n",
    "            h_text_pooled = torch.cat(pool_chunks, dim=0).numpy().astype(SAVE_DTYPE)  # [N, D_MODEL]\n",
    "\n",
    "        out_path = os.path.join(PD_Spontaneous_OUT_DIR, f\"{base}_h_text_selfattn.npz\")\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_text=h_text,\n",
    "            attention_mask=out_mask,                 # keep the same mask\n",
    "            h_text_pooled=(h_text_pooled if SAVE_POOLED else None),\n",
    "            d_model=np.array(D_MODEL),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_text_npz=os.path.basename(tpath),\n",
    "        )\n",
    "        print(f\"Saved: {out_path} | h_text {h_text.shape}\" + (f\", pooled {h_text_pooled.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28756b6c",
   "metadata": {},
   "source": [
    "# Self Attention to get the h_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c707177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 NPZ files to process.\n",
      "[info] initialized encoder: d_in=768, d_model=768, heads=8, layers=2\n",
      "[ok]   ID00_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7550, 768), pooled (1, 768)\n",
      "[ok]   ID01_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8200, 768), pooled (1, 768)\n",
      "[ok]   ID03_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6973, 768), pooled (1, 768)\n",
      "[ok]   ID05_hc_0_0_0_hubert_feats.npz -> h_audio (1, 5543, 768), pooled (1, 768)\n",
      "[ok]   ID08_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7300, 768), pooled (1, 768)\n",
      "[ok]   ID09_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6269, 768), pooled (1, 768)\n",
      "[ok]   ID10_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6834, 768), pooled (1, 768)\n",
      "[ok]   ID11_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7415, 768), pooled (1, 768)\n",
      "[ok]   ID12_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6522, 768), pooled (1, 768)\n",
      "[ok]   ID14_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8415, 768), pooled (1, 768)\n",
      "[ok]   ID15_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8526, 768), pooled (1, 768)\n",
      "[ok]   ID19_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7058, 768), pooled (1, 768)\n",
      "[ok]   ID21_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6106, 768), pooled (1, 768)\n",
      "[ok]   ID22_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8366, 768), pooled (1, 768)\n",
      "[ok]   ID23_hc_0_0_0_hubert_feats.npz -> h_audio (1, 10173, 768), pooled (1, 768)\n",
      "[ok]   ID25_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8712, 768), pooled (1, 768)\n",
      "[ok]   ID26_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7681, 768), pooled (1, 768)\n",
      "[ok]   ID28_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7284, 768), pooled (1, 768)\n",
      "[ok]   ID31_hc_0_1_1_hubert_feats.npz -> h_audio (1, 9642, 768), pooled (1, 768)\n",
      "[ok]   ID35_hc_0_0_0_hubert_feats.npz -> h_audio (1, 4658, 768), pooled (1, 768)\n",
      "[ok]   ID36_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7924, 768), pooled (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===================== Config =====================\n",
    "AUDIO_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/hubert_audio/HC_ReadText_hubert_features\"   # <-- change to your folder with *_hubert_feats.npz\n",
    "OUT_DIR       = os.path.join(AUDIO_NPZ_DIR, \"..\", \"h_audio_selfattn\")\n",
    "PATTERNS      = [\"*_hubert_feats.npz\", \"*_audio_feats.npz\", \"*.npz\"]  # will auto-pick the right keys\n",
    "\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_N     = 64             # sequences per forward pass when input is [N, L, D]\n",
    "D_MODEL     = None           # None -> use d_in; or set e.g. 512/768/1024 to project\n",
    "N_HEADS     = 8\n",
    "N_LAYERS    = 2\n",
    "D_FF        = 2048\n",
    "DROPOUT     = 0.1\n",
    "SAVE_DTYPE  = np.float32     # use np.float16 to halve disk space\n",
    "SAVE_POOLED = True\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "assert os.path.isdir(AUDIO_NPZ_DIR), f\"Missing input dir: {AUDIO_NPZ_DIR}\"\n",
    "assert os.path.isdir(OUT_DIR),       f\"Missing output dir: {OUT_DIR}\"\n",
    "\n",
    "# ================= Model =================\n",
    "class SelfAttentionAudioEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, audio_embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "        audio_embeddings: [B, L, d_in]  or [L, d_in]\n",
    "        attention_mask:   [B, L]        or [L]  (1/True=valid, 0/False=pad)\n",
    "        returns:          [B, L, d_model]\n",
    "        \"\"\"\n",
    "        if audio_embeddings.dim() == 2:\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1:\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        x = self.in_proj(audio_embeddings)                 # [B, L, d_model]\n",
    "        key_padding_mask = ~attention_mask.bool()          # True=ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    # x: [B, L, D], mask: [B, L] (True=valid)\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)      # [B, D]\n",
    "\n",
    "# ================= I/O helpers =================\n",
    "def pick_feat_key(keys):\n",
    "    for k in [\"hubert_embeddings\", \"audio_embeddings\", \"features\", \"hidden_states\"]:\n",
    "        if k in keys: return k\n",
    "    raise KeyError(f\"No audio feature key found in {sorted(keys)}\")\n",
    "\n",
    "def load_audio_npz(path):\n",
    "    with np.load(path, allow_pickle=True, mmap_mode=\"r\") as arr:\n",
    "        fk = pick_feat_key(arr.files)\n",
    "        feats = arr[fk]  # np.ndarray: [L, D] or [N, L, D]\n",
    "        if \"attention_mask\" in arr.files:\n",
    "            mask = arr[\"attention_mask\"]\n",
    "        elif feats.ndim == 3:\n",
    "            # if no mask, assume all valid\n",
    "            mask = np.ones((feats.shape[0], feats.shape[1]), dtype=bool)\n",
    "        else:\n",
    "            mask = np.ones((feats.shape[0],), dtype=bool)\n",
    "    return feats, mask  # numpy arrays\n",
    "\n",
    "# ================= Main =================\n",
    "def main():\n",
    "    # collect files\n",
    "    paths = []\n",
    "    for pat in PATTERNS:\n",
    "        paths.extend(glob.glob(os.path.join(AUDIO_NPZ_DIR, pat)))\n",
    "    # de-dup and sort\n",
    "    paths = sorted(set(paths))\n",
    "    print(f\"Found {len(paths)} NPZ files to process.\")\n",
    "\n",
    "    encoder = None\n",
    "    d_in_cached = None\n",
    "    d_model = None\n",
    "\n",
    "    for p in paths:\n",
    "        base = os.path.splitext(os.path.basename(p))[0]\n",
    "        out_path = os.path.join(OUT_DIR, f\"{base.replace('_hubert_feats','').replace('_audio_feats','')}_h_audio_selfattn.npz\")\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"[skip] {os.path.basename(out_path)} (exists)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            feats_np, mask_np = load_audio_npz(p)  # feats: [L,D] or [N,L,D]; mask: [L] or [N,L]\n",
    "        except Exception as e:\n",
    "            print(f\"[err]  {os.path.basename(p)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # standardize to torch\n",
    "        feats = torch.from_numpy(feats_np).float()\n",
    "        mask  = torch.from_numpy(mask_np).bool()\n",
    "\n",
    "        # infer dimensions\n",
    "        if feats.dim() == 2:\n",
    "            L, d_in = feats.shape\n",
    "            N = 1\n",
    "        elif feats.dim() == 3:\n",
    "            N, L, d_in = feats.shape\n",
    "        else:\n",
    "            print(f\"[err]  {os.path.basename(p)}: unexpected feature shape {feats.shape}\")\n",
    "            continue\n",
    "\n",
    "        # build encoder once (or rebuild if d_in changes across files)\n",
    "        if d_in_cached != d_in:\n",
    "            d_in_cached = d_in\n",
    "            d_model = d_in if D_MODEL is None else int(D_MODEL)\n",
    "            encoder = SelfAttentionAudioEncoder(\n",
    "                d_in=d_in, d_model=d_model, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "            print(f\"[info] initialized encoder: d_in={d_in}, d_model={d_model}, heads={N_HEADS}, layers={N_LAYERS}\")\n",
    "\n",
    "        # run encoder\n",
    "        with torch.no_grad():\n",
    "            if feats.dim() == 2:\n",
    "                h = encoder(feats.to(DEVICE), mask.to(DEVICE))                # [1, L, d_model]\n",
    "                pooled = masked_mean(h, mask.unsqueeze(0).to(DEVICE)) if SAVE_POOLED else None  # [1, d_model]\n",
    "                h_np   = h.cpu().numpy().astype(SAVE_DTYPE)                   # [1, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                   # [L]\n",
    "                poolnp = None if pooled is None else pooled.cpu().numpy().astype(SAVE_DTYPE)  # [1, d_model]\n",
    "            else:\n",
    "                # [N, L, D] -> process in chunks along N\n",
    "                h_chunks, pool_chunks = [], []\n",
    "                for i in range(0, N, BATCH_N):\n",
    "                    xb = feats[i:i+BATCH_N].to(DEVICE)                        # [b, L, d_in]\n",
    "                    mb = mask[i:i+BATCH_N].to(DEVICE)                         # [b, L]\n",
    "                    hb = encoder(xb, mb)                                      # [b, L, d_model]\n",
    "                    h_chunks.append(hb.cpu())\n",
    "                    if SAVE_POOLED:\n",
    "                        pool_chunks.append(masked_mean(hb, mb).cpu())         # [b, d_model]\n",
    "                h_np   = torch.cat(h_chunks, 0).numpy().astype(SAVE_DTYPE)    # [N, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                    # [N, L]\n",
    "                poolnp = None if not SAVE_POOLED else torch.cat(pool_chunks, 0).numpy().astype(SAVE_DTYPE)  # [N, d_model]\n",
    "\n",
    "        # save\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_audio=h_np,\n",
    "            attention_mask=m_np,\n",
    "            h_audio_pooled=poolnp,\n",
    "            d_model=np.array(d_model),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_audio_npz=os.path.basename(p),\n",
    "        )\n",
    "        print(f\"[ok]   {os.path.basename(p)} -> h_audio {h_np.shape}\" + (f\", pooled {poolnp.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d56d2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 NPZ files to process.\n",
      "[info] initialized encoder: d_in=768, d_model=768, heads=8, layers=2\n",
      "[ok]   ID00_hc_0_0_0_hubert_feats.npz -> h_audio (1, 5988, 768), pooled (1, 768)\n",
      "[ok]   ID01_hc_0_0_0_hubert_feats.npz -> h_audio (1, 5850, 768), pooled (1, 768)\n",
      "[ok]   ID03_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7700, 768), pooled (1, 768)\n",
      "[ok]   ID05_hc_0_0_0_hubert_feats.npz -> h_audio (1, 9218, 768), pooled (1, 768)\n",
      "[ok]   ID08_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6414, 768), pooled (1, 768)\n",
      "[ok]   ID09_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7060, 768), pooled (1, 768)\n",
      "[ok]   ID10_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8467, 768), pooled (1, 768)\n",
      "[ok]   ID11_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8305, 768), pooled (1, 768)\n",
      "[ok]   ID12_hc_0_0_0_hubert_feats.npz -> h_audio (1, 8255, 768), pooled (1, 768)\n",
      "[ok]   ID14_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6938, 768), pooled (1, 768)\n",
      "[ok]   ID15_hc_0_0_0_hubert_feats.npz -> h_audio (1, 11024, 768), pooled (1, 768)\n",
      "[ok]   ID19_hc_0_0_0_hubert_feats.npz -> h_audio (1, 4594, 768), pooled (1, 768)\n",
      "[ok]   ID21_hc_0_0_0_hubert_feats.npz -> h_audio (1, 4452, 768), pooled (1, 768)\n",
      "[ok]   ID22hc_0_0_0_hubert_feats.npz -> h_audio (1, 7506, 768), pooled (1, 768)\n",
      "[ok]   ID23_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7189, 768), pooled (1, 768)\n",
      "[ok]   ID25_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7083, 768), pooled (1, 768)\n",
      "[ok]   ID26_hc_0_0_0_hubert_feats.npz -> h_audio (1, 6510, 768), pooled (1, 768)\n",
      "[ok]   ID28_hc_0_0_0_hubert_feats.npz -> h_audio (1, 5812, 768), pooled (1, 768)\n",
      "[ok]   ID31_hc_0_1_1_hubert_feats.npz -> h_audio (1, 3815, 768), pooled (1, 768)\n",
      "[ok]   ID35_hc_0_0_0_hubert_feats.npz -> h_audio (1, 7418, 768), pooled (1, 768)\n",
      "[ok]   ID36_hc_0_0_0_hubert_feats.npz -> h_audio (1, 5665, 768), pooled (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===================== Config =====================\n",
    "HC_Spontaneous_AUDIO_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/hubert_audio/HC_Spontaneous_hubert_features\"   # <-- change to your folder with *_hubert_feats.npz\n",
    "HC_Spontaneous_OUT_DIR       = os.path.join(HC_Spontaneous_AUDIO_NPZ_DIR, \"..\", \"HC_Spontaneous_h_audio_selfattn\")\n",
    "PATTERNS      = [\"*_hubert_feats.npz\", \"*_audio_feats.npz\", \"*.npz\"]  # will auto-pick the right keys\n",
    "\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_N     = 64             # sequences per forward pass when input is [N, L, D]\n",
    "D_MODEL     = None           # None -> use d_in; or set e.g. 512/768/1024 to project\n",
    "N_HEADS     = 8\n",
    "N_LAYERS    = 2\n",
    "D_FF        = 2048\n",
    "DROPOUT     = 0.1\n",
    "SAVE_DTYPE  = np.float32     # use np.float16 to halve disk space\n",
    "SAVE_POOLED = True\n",
    "\n",
    "os.makedirs(HC_Spontaneous_OUT_DIR, exist_ok=True)\n",
    "assert os.path.isdir(HC_Spontaneous_AUDIO_NPZ_DIR), f\"Missing input dir: {HC_Spontaneous_AUDIO_NPZ_DIR}\"\n",
    "assert os.path.isdir(HC_Spontaneous_OUT_DIR),       f\"Missing output dir: {HC_Spontaneous_OUT_DIR}\"\n",
    "\n",
    "# ================= Model =================\n",
    "class SelfAttentionAudioEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, audio_embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "        audio_embeddings: [B, L, d_in]  or [L, d_in]\n",
    "        attention_mask:   [B, L]        or [L]  (1/True=valid, 0/False=pad)\n",
    "        returns:          [B, L, d_model]\n",
    "        \"\"\"\n",
    "        if audio_embeddings.dim() == 2:\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1:\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        x = self.in_proj(audio_embeddings)                 # [B, L, d_model]\n",
    "        key_padding_mask = ~attention_mask.bool()          # True=ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    # x: [B, L, D], mask: [B, L] (True=valid)\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)      # [B, D]\n",
    "\n",
    "# ================= I/O helpers =================\n",
    "def pick_feat_key(keys):\n",
    "    for k in [\"hubert_embeddings\", \"audio_embeddings\", \"features\", \"hidden_states\"]:\n",
    "        if k in keys: return k\n",
    "    raise KeyError(f\"No audio feature key found in {sorted(keys)}\")\n",
    "\n",
    "def load_audio_npz(path):\n",
    "    with np.load(path, allow_pickle=True, mmap_mode=\"r\") as arr:\n",
    "        fk = pick_feat_key(arr.files)\n",
    "        feats = arr[fk]  # np.ndarray: [L, D] or [N, L, D]\n",
    "        if \"attention_mask\" in arr.files:\n",
    "            mask = arr[\"attention_mask\"]\n",
    "        elif feats.ndim == 3:\n",
    "            # if no mask, assume all valid\n",
    "            mask = np.ones((feats.shape[0], feats.shape[1]), dtype=bool)\n",
    "        else:\n",
    "            mask = np.ones((feats.shape[0],), dtype=bool)\n",
    "    return feats, mask  # numpy arrays\n",
    "\n",
    "# ================= Main =================\n",
    "def main():\n",
    "    # collect files\n",
    "    paths = []\n",
    "    for pat in PATTERNS:\n",
    "        paths.extend(glob.glob(os.path.join(HC_Spontaneous_AUDIO_NPZ_DIR, pat)))\n",
    "    # de-dup and sort\n",
    "    paths = sorted(set(paths))\n",
    "    print(f\"Found {len(paths)} NPZ files to process.\")\n",
    "\n",
    "    encoder = None\n",
    "    d_in_cached = None\n",
    "    d_model = None\n",
    "\n",
    "    for p in paths:\n",
    "        base = os.path.splitext(os.path.basename(p))[0]\n",
    "        out_path = os.path.join(HC_Spontaneous_OUT_DIR, f\"{base.replace('_hubert_feats','').replace('_audio_feats','')}_h_audio_selfattn.npz\")\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"[skip] {os.path.basename(out_path)} (exists)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            feats_np, mask_np = load_audio_npz(p)  # feats: [L,D] or [N,L,D]; mask: [L] or [N,L]\n",
    "        except Exception as e:\n",
    "            print(f\"[err]  {os.path.basename(p)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # standardize to torch\n",
    "        feats = torch.from_numpy(feats_np).float()\n",
    "        mask  = torch.from_numpy(mask_np).bool()\n",
    "\n",
    "        # infer dimensions\n",
    "        if feats.dim() == 2:\n",
    "            L, d_in = feats.shape\n",
    "            N = 1\n",
    "        elif feats.dim() == 3:\n",
    "            N, L, d_in = feats.shape\n",
    "        else:\n",
    "            print(f\"[err]  {os.path.basename(p)}: unexpected feature shape {feats.shape}\")\n",
    "            continue\n",
    "\n",
    "        # build encoder once (or rebuild if d_in changes across files)\n",
    "        if d_in_cached != d_in:\n",
    "            d_in_cached = d_in\n",
    "            d_model = d_in if D_MODEL is None else int(D_MODEL)\n",
    "            encoder = SelfAttentionAudioEncoder(\n",
    "                d_in=d_in, d_model=d_model, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "            print(f\"[info] initialized encoder: d_in={d_in}, d_model={d_model}, heads={N_HEADS}, layers={N_LAYERS}\")\n",
    "\n",
    "        # run encoder\n",
    "        with torch.no_grad():\n",
    "            if feats.dim() == 2:\n",
    "                h = encoder(feats.to(DEVICE), mask.to(DEVICE))                # [1, L, d_model]\n",
    "                pooled = masked_mean(h, mask.unsqueeze(0).to(DEVICE)) if SAVE_POOLED else None  # [1, d_model]\n",
    "                h_np   = h.cpu().numpy().astype(SAVE_DTYPE)                   # [1, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                   # [L]\n",
    "                poolnp = None if pooled is None else pooled.cpu().numpy().astype(SAVE_DTYPE)  # [1, d_model]\n",
    "            else:\n",
    "                # [N, L, D] -> process in chunks along N\n",
    "                h_chunks, pool_chunks = [], []\n",
    "                for i in range(0, N, BATCH_N):\n",
    "                    xb = feats[i:i+BATCH_N].to(DEVICE)                        # [b, L, d_in]\n",
    "                    mb = mask[i:i+BATCH_N].to(DEVICE)                         # [b, L]\n",
    "                    hb = encoder(xb, mb)                                      # [b, L, d_model]\n",
    "                    h_chunks.append(hb.cpu())\n",
    "                    if SAVE_POOLED:\n",
    "                        pool_chunks.append(masked_mean(hb, mb).cpu())         # [b, d_model]\n",
    "                h_np   = torch.cat(h_chunks, 0).numpy().astype(SAVE_DTYPE)    # [N, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                    # [N, L]\n",
    "                poolnp = None if not SAVE_POOLED else torch.cat(pool_chunks, 0).numpy().astype(SAVE_DTYPE)  # [N, d_model]\n",
    "\n",
    "        # save\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_audio=h_np,\n",
    "            attention_mask=m_np,\n",
    "            h_audio_pooled=poolnp,\n",
    "            d_model=np.array(d_model),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_audio_npz=os.path.basename(p),\n",
    "        )\n",
    "        print(f\"[ok]   {os.path.basename(p)} -> h_audio {h_np.shape}\" + (f\", pooled {poolnp.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fdf1b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 NPZ files to process.\n",
      "[info] initialized encoder: d_in=768, d_model=768, heads=8, layers=2\n",
      "[ok]   ID02_pd_2_0_0_hubert_feats.npz -> h_audio (1, 7872, 768), pooled (1, 768)\n",
      "[ok]   ID04_pd_2_0_1_hubert_feats.npz -> h_audio (1, 6119, 768), pooled (1, 768)\n",
      "[ok]   ID06_pd_3_1_1_hubert_feats.npz -> h_audio (1, 8940, 768), pooled (1, 768)\n",
      "[ok]   ID07_pd_2_0_0_hubert_feats.npz -> h_audio (1, 7379, 768), pooled (1, 768)\n",
      "[ok]   ID13_pd_3_2_2_hubert_feats.npz -> h_audio (1, 4664, 768), pooled (1, 768)\n",
      "[ok]   ID16_pd_2_0_0_hubert_feats.npz -> h_audio (1, 8357, 768), pooled (1, 768)\n",
      "[ok]   ID17_pd_2_1_0_hubert_feats.npz -> h_audio (1, 5467, 768), pooled (1, 768)\n",
      "[ok]   ID18_pd_4_3_3_hubert_feats.npz -> h_audio (1, 4284, 768), pooled (1, 768)\n",
      "[ok]   ID20_pd_3_0_1_hubert_feats.npz -> h_audio (1, 7049, 768), pooled (1, 768)\n",
      "[ok]   ID24_pd_2_0_0_hubert_feats.npz -> h_audio (1, 7092, 768), pooled (1, 768)\n",
      "[ok]   ID27_pd_4_1_1_hubert_feats.npz -> h_audio (1, 4186, 768), pooled (1, 768)\n",
      "[ok]   ID29_pd_3_1_2_hubert_feats.npz -> h_audio (1, 7758, 768), pooled (1, 768)\n",
      "[ok]   ID30_pd_2_1_1_hubert_feats.npz -> h_audio (1, 6608, 768), pooled (1, 768)\n",
      "[ok]   ID32_pd_3_1_1_hubert_feats.npz -> h_audio (1, 3649, 768), pooled (1, 768)\n",
      "[ok]   ID33_pd_3_2_2_hubert_feats.npz -> h_audio (1, 5915, 768), pooled (1, 768)\n",
      "[ok]   ID34_pd_2_0_0_hubert_feats.npz -> h_audio (1, 6391, 768), pooled (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===================== Config =====================\n",
    "PD_ReadText_AUDIO_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/hubert_audio/PD_ReadText_hubert_features\"   # <-- change to your folder with *_hubert_feats.npz\n",
    "PD_ReadText_OUT_DIR       = os.path.join(PD_ReadText_AUDIO_NPZ_DIR, \"..\", \"PD_ReadText_h_audio_selfattn\")\n",
    "PATTERNS      = [\"*_hubert_feats.npz\", \"*_audio_feats.npz\", \"*.npz\"]  # will auto-pick the right keys\n",
    "\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_N     = 64             # sequences per forward pass when input is [N, L, D]\n",
    "D_MODEL     = None           # None -> use d_in; or set e.g. 512/768/1024 to project\n",
    "N_HEADS     = 8\n",
    "N_LAYERS    = 2\n",
    "D_FF        = 2048\n",
    "DROPOUT     = 0.1\n",
    "SAVE_DTYPE  = np.float32     # use np.float16 to halve disk space\n",
    "SAVE_POOLED = True\n",
    "\n",
    "os.makedirs(PD_ReadText_OUT_DIR, exist_ok=True)\n",
    "assert os.path.isdir(PD_ReadText_AUDIO_NPZ_DIR), f\"Missing input dir: {PD_ReadText_AUDIO_NPZ_DIR}\"\n",
    "assert os.path.isdir(PD_ReadText_OUT_DIR),       f\"Missing output dir: {PD_ReadText_OUT_DIR}\"\n",
    "\n",
    "# ================= Model =================\n",
    "class SelfAttentionAudioEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, audio_embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "        audio_embeddings: [B, L, d_in]  or [L, d_in]\n",
    "        attention_mask:   [B, L]        or [L]  (1/True=valid, 0/False=pad)\n",
    "        returns:          [B, L, d_model]\n",
    "        \"\"\"\n",
    "        if audio_embeddings.dim() == 2:\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1:\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        x = self.in_proj(audio_embeddings)                 # [B, L, d_model]\n",
    "        key_padding_mask = ~attention_mask.bool()          # True=ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    # x: [B, L, D], mask: [B, L] (True=valid)\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)      # [B, D]\n",
    "\n",
    "# ================= I/O helpers =================\n",
    "def pick_feat_key(keys):\n",
    "    for k in [\"hubert_embeddings\", \"audio_embeddings\", \"features\", \"hidden_states\"]:\n",
    "        if k in keys: return k\n",
    "    raise KeyError(f\"No audio feature key found in {sorted(keys)}\")\n",
    "\n",
    "def load_audio_npz(path):\n",
    "    with np.load(path, allow_pickle=True, mmap_mode=\"r\") as arr:\n",
    "        fk = pick_feat_key(arr.files)\n",
    "        feats = arr[fk]  # np.ndarray: [L, D] or [N, L, D]\n",
    "        if \"attention_mask\" in arr.files:\n",
    "            mask = arr[\"attention_mask\"]\n",
    "        elif feats.ndim == 3:\n",
    "            # if no mask, assume all valid\n",
    "            mask = np.ones((feats.shape[0], feats.shape[1]), dtype=bool)\n",
    "        else:\n",
    "            mask = np.ones((feats.shape[0],), dtype=bool)\n",
    "    return feats, mask  # numpy arrays\n",
    "\n",
    "# ================= Main =================\n",
    "def main():\n",
    "    # collect files\n",
    "    paths = []\n",
    "    for pat in PATTERNS:\n",
    "        paths.extend(glob.glob(os.path.join(PD_ReadText_AUDIO_NPZ_DIR, pat)))\n",
    "    # de-dup and sort\n",
    "    paths = sorted(set(paths))\n",
    "    print(f\"Found {len(paths)} NPZ files to process.\")\n",
    "\n",
    "    encoder = None\n",
    "    d_in_cached = None\n",
    "    d_model = None\n",
    "\n",
    "    for p in paths:\n",
    "        base = os.path.splitext(os.path.basename(p))[0]\n",
    "        out_path = os.path.join(PD_ReadText_OUT_DIR, f\"{base.replace('_hubert_feats','').replace('_audio_feats','')}_h_audio_selfattn.npz\")\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"[skip] {os.path.basename(out_path)} (exists)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            feats_np, mask_np = load_audio_npz(p)  # feats: [L,D] or [N,L,D]; mask: [L] or [N,L]\n",
    "        except Exception as e:\n",
    "            print(f\"[err]  {os.path.basename(p)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # standardize to torch\n",
    "        feats = torch.from_numpy(feats_np).float()\n",
    "        mask  = torch.from_numpy(mask_np).bool()\n",
    "\n",
    "        # infer dimensions\n",
    "        if feats.dim() == 2:\n",
    "            L, d_in = feats.shape\n",
    "            N = 1\n",
    "        elif feats.dim() == 3:\n",
    "            N, L, d_in = feats.shape\n",
    "        else:\n",
    "            print(f\"[err]  {os.path.basename(p)}: unexpected feature shape {feats.shape}\")\n",
    "            continue\n",
    "\n",
    "        # build encoder once (or rebuild if d_in changes across files)\n",
    "        if d_in_cached != d_in:\n",
    "            d_in_cached = d_in\n",
    "            d_model = d_in if D_MODEL is None else int(D_MODEL)\n",
    "            encoder = SelfAttentionAudioEncoder(\n",
    "                d_in=d_in, d_model=d_model, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "            print(f\"[info] initialized encoder: d_in={d_in}, d_model={d_model}, heads={N_HEADS}, layers={N_LAYERS}\")\n",
    "\n",
    "        # run encoder\n",
    "        with torch.no_grad():\n",
    "            if feats.dim() == 2:\n",
    "                h = encoder(feats.to(DEVICE), mask.to(DEVICE))                # [1, L, d_model]\n",
    "                pooled = masked_mean(h, mask.unsqueeze(0).to(DEVICE)) if SAVE_POOLED else None  # [1, d_model]\n",
    "                h_np   = h.cpu().numpy().astype(SAVE_DTYPE)                   # [1, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                   # [L]\n",
    "                poolnp = None if pooled is None else pooled.cpu().numpy().astype(SAVE_DTYPE)  # [1, d_model]\n",
    "            else:\n",
    "                # [N, L, D] -> process in chunks along N\n",
    "                h_chunks, pool_chunks = [], []\n",
    "                for i in range(0, N, BATCH_N):\n",
    "                    xb = feats[i:i+BATCH_N].to(DEVICE)                        # [b, L, d_in]\n",
    "                    mb = mask[i:i+BATCH_N].to(DEVICE)                         # [b, L]\n",
    "                    hb = encoder(xb, mb)                                      # [b, L, d_model]\n",
    "                    h_chunks.append(hb.cpu())\n",
    "                    if SAVE_POOLED:\n",
    "                        pool_chunks.append(masked_mean(hb, mb).cpu())         # [b, d_model]\n",
    "                h_np   = torch.cat(h_chunks, 0).numpy().astype(SAVE_DTYPE)    # [N, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                    # [N, L]\n",
    "                poolnp = None if not SAVE_POOLED else torch.cat(pool_chunks, 0).numpy().astype(SAVE_DTYPE)  # [N, d_model]\n",
    "\n",
    "        # save\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_audio=h_np,\n",
    "            attention_mask=m_np,\n",
    "            h_audio_pooled=poolnp,\n",
    "            d_model=np.array(d_model),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_audio_npz=os.path.basename(p),\n",
    "        )\n",
    "        print(f\"[ok]   {os.path.basename(p)} -> h_audio {h_np.shape}\" + (f\", pooled {poolnp.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66646609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 NPZ files to process.\n",
      "[info] initialized encoder: d_in=768, d_model=768, heads=8, layers=2\n",
      "[ok]   ID02_pd_2_0_0_hubert_feats.npz -> h_audio (1, 9462, 768), pooled (1, 768)\n",
      "[ok]   ID04_pd_2_0_1_hubert_feats.npz -> h_audio (1, 7340, 768), pooled (1, 768)\n",
      "[ok]   ID06_pd_3_1_1_hubert_feats.npz -> h_audio (1, 6411, 768), pooled (1, 768)\n",
      "[ok]   ID07_pd_2_0_0_hubert_feats.npz -> h_audio (1, 10457, 768), pooled (1, 768)\n",
      "[ok]   ID13_pd_3_2_2_hubert_feats.npz -> h_audio (1, 9966, 768), pooled (1, 768)\n",
      "[ok]   ID16_pd_2_0_0_hubert_feats.npz -> h_audio (1, 7824, 768), pooled (1, 768)\n",
      "[ok]   ID17_pd_2_1_0_hubert_feats.npz -> h_audio (1, 7712, 768), pooled (1, 768)\n",
      "[ok]   ID20_pd_3_0_1_hubert_feats.npz -> h_audio (1, 5511, 768), pooled (1, 768)\n",
      "[ok]   ID24_pd_2_0_0_hubert_feats.npz -> h_audio (1, 6974, 768), pooled (1, 768)\n",
      "[ok]   ID27_pd_4_1_1_hubert_feats.npz -> h_audio (1, 8609, 768), pooled (1, 768)\n",
      "[ok]   ID29_pd_3_1_2_hubert_feats.npz -> h_audio (1, 7033, 768), pooled (1, 768)\n",
      "[ok]   ID30_pd_2_1_1_hubert_feats.npz -> h_audio (1, 6623, 768), pooled (1, 768)\n",
      "[ok]   ID32_pd_3_1_1_hubert_feats.npz -> h_audio (1, 5265, 768), pooled (1, 768)\n",
      "[ok]   ID33_pd_3_2_2_hubert_feats.npz -> h_audio (1, 6140, 768), pooled (1, 768)\n",
      "[ok]   ID34_pd_2_0_0_hubert_feats.npz -> h_audio (1, 6092, 768), pooled (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===================== Config =====================\n",
    "PD_Spontaneous_AUDIO_NPZ_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/hubert_audio/PD_Spontaneous_hubert_features\"   # <-- change to your folder with *_hubert_feats.npz\n",
    "PD_Spontaneous_OUT_DIR       = os.path.join(PD_Spontaneous_AUDIO_NPZ_DIR, \"..\", \"PD_Spontaneous_h_audio_selfattn\")\n",
    "PATTERNS      = [\"*_hubert_feats.npz\", \"*_audio_feats.npz\", \"*.npz\"]  # will auto-pick the right keys\n",
    "\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_N     = 64             # sequences per forward pass when input is [N, L, D]\n",
    "D_MODEL     = None           # None -> use d_in; or set e.g. 512/768/1024 to project\n",
    "N_HEADS     = 8\n",
    "N_LAYERS    = 2\n",
    "D_FF        = 2048\n",
    "DROPOUT     = 0.1\n",
    "SAVE_DTYPE  = np.float32     # use np.float16 to halve disk space\n",
    "SAVE_POOLED = True\n",
    "\n",
    "os.makedirs(PD_Spontaneous_OUT_DIR, exist_ok=True)\n",
    "assert os.path.isdir(PD_Spontaneous_AUDIO_NPZ_DIR), f\"Missing input dir: {PD_Spontaneous_AUDIO_NPZ_DIR}\"\n",
    "assert os.path.isdir(PD_Spontaneous_OUT_DIR),       f\"Missing output dir: {PD_Spontaneous_OUT_DIR}\"\n",
    "\n",
    "# ================= Model =================\n",
    "class SelfAttentionAudioEncoder(nn.Module):\n",
    "    def __init__(self, d_in=768, d_model=768, n_heads=8, n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, audio_embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "        audio_embeddings: [B, L, d_in]  or [L, d_in]\n",
    "        attention_mask:   [B, L]        or [L]  (1/True=valid, 0/False=pad)\n",
    "        returns:          [B, L, d_model]\n",
    "        \"\"\"\n",
    "        if audio_embeddings.dim() == 2:\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1:\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        x = self.in_proj(audio_embeddings)                 # [B, L, d_model]\n",
    "        key_padding_mask = ~attention_mask.bool()          # True=ignore\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill((~attention_mask.bool()).unsqueeze(-1), 0.0)  # tidy pads\n",
    "        return x\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    # x: [B, L, D], mask: [B, L] (True=valid)\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    return (x * m).sum(1) / m.sum(1).clamp(min=1e-6)      # [B, D]\n",
    "\n",
    "# ================= I/O helpers =================\n",
    "def pick_feat_key(keys):\n",
    "    for k in [\"hubert_embeddings\", \"audio_embeddings\", \"features\", \"hidden_states\"]:\n",
    "        if k in keys: return k\n",
    "    raise KeyError(f\"No audio feature key found in {sorted(keys)}\")\n",
    "\n",
    "def load_audio_npz(path):\n",
    "    with np.load(path, allow_pickle=True, mmap_mode=\"r\") as arr:\n",
    "        fk = pick_feat_key(arr.files)\n",
    "        feats = arr[fk]  # np.ndarray: [L, D] or [N, L, D]\n",
    "        if \"attention_mask\" in arr.files:\n",
    "            mask = arr[\"attention_mask\"]\n",
    "        elif feats.ndim == 3:\n",
    "            # if no mask, assume all valid\n",
    "            mask = np.ones((feats.shape[0], feats.shape[1]), dtype=bool)\n",
    "        else:\n",
    "            mask = np.ones((feats.shape[0],), dtype=bool)\n",
    "    return feats, mask  # numpy arrays\n",
    "\n",
    "# ================= Main =================\n",
    "def main():\n",
    "    # collect files\n",
    "    paths = []\n",
    "    for pat in PATTERNS:\n",
    "        paths.extend(glob.glob(os.path.join(PD_Spontaneous_AUDIO_NPZ_DIR, pat)))\n",
    "    # de-dup and sort\n",
    "    paths = sorted(set(paths))\n",
    "    print(f\"Found {len(paths)} NPZ files to process.\")\n",
    "\n",
    "    encoder = None\n",
    "    d_in_cached = None\n",
    "    d_model = None\n",
    "\n",
    "    for p in paths:\n",
    "        base = os.path.splitext(os.path.basename(p))[0]\n",
    "        out_path = os.path.join(PD_Spontaneous_OUT_DIR, f\"{base.replace('_hubert_feats','').replace('_audio_feats','')}_h_audio_selfattn.npz\")\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"[skip] {os.path.basename(out_path)} (exists)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            feats_np, mask_np = load_audio_npz(p)  # feats: [L,D] or [N,L,D]; mask: [L] or [N,L]\n",
    "        except Exception as e:\n",
    "            print(f\"[err]  {os.path.basename(p)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # standardize to torch\n",
    "        feats = torch.from_numpy(feats_np).float()\n",
    "        mask  = torch.from_numpy(mask_np).bool()\n",
    "\n",
    "        # infer dimensions\n",
    "        if feats.dim() == 2:\n",
    "            L, d_in = feats.shape\n",
    "            N = 1\n",
    "        elif feats.dim() == 3:\n",
    "            N, L, d_in = feats.shape\n",
    "        else:\n",
    "            print(f\"[err]  {os.path.basename(p)}: unexpected feature shape {feats.shape}\")\n",
    "            continue\n",
    "\n",
    "        # build encoder once (or rebuild if d_in changes across files)\n",
    "        if d_in_cached != d_in:\n",
    "            d_in_cached = d_in\n",
    "            d_model = d_in if D_MODEL is None else int(D_MODEL)\n",
    "            encoder = SelfAttentionAudioEncoder(\n",
    "                d_in=d_in, d_model=d_model, n_heads=N_HEADS, n_layers=N_LAYERS, d_ff=D_FF, dropout=DROPOUT\n",
    "            ).to(DEVICE).eval()\n",
    "            print(f\"[info] initialized encoder: d_in={d_in}, d_model={d_model}, heads={N_HEADS}, layers={N_LAYERS}\")\n",
    "\n",
    "        # run encoder\n",
    "        with torch.no_grad():\n",
    "            if feats.dim() == 2:\n",
    "                h = encoder(feats.to(DEVICE), mask.to(DEVICE))                # [1, L, d_model]\n",
    "                pooled = masked_mean(h, mask.unsqueeze(0).to(DEVICE)) if SAVE_POOLED else None  # [1, d_model]\n",
    "                h_np   = h.cpu().numpy().astype(SAVE_DTYPE)                   # [1, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                   # [L]\n",
    "                poolnp = None if pooled is None else pooled.cpu().numpy().astype(SAVE_DTYPE)  # [1, d_model]\n",
    "            else:\n",
    "                # [N, L, D] -> process in chunks along N\n",
    "                h_chunks, pool_chunks = [], []\n",
    "                for i in range(0, N, BATCH_N):\n",
    "                    xb = feats[i:i+BATCH_N].to(DEVICE)                        # [b, L, d_in]\n",
    "                    mb = mask[i:i+BATCH_N].to(DEVICE)                         # [b, L]\n",
    "                    hb = encoder(xb, mb)                                      # [b, L, d_model]\n",
    "                    h_chunks.append(hb.cpu())\n",
    "                    if SAVE_POOLED:\n",
    "                        pool_chunks.append(masked_mean(hb, mb).cpu())         # [b, d_model]\n",
    "                h_np   = torch.cat(h_chunks, 0).numpy().astype(SAVE_DTYPE)    # [N, L, d_model]\n",
    "                m_np   = mask.cpu().numpy()                                    # [N, L]\n",
    "                poolnp = None if not SAVE_POOLED else torch.cat(pool_chunks, 0).numpy().astype(SAVE_DTYPE)  # [N, d_model]\n",
    "\n",
    "        # save\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            h_audio=h_np,\n",
    "            attention_mask=m_np,\n",
    "            h_audio_pooled=poolnp,\n",
    "            d_model=np.array(d_model),\n",
    "            n_heads=np.array(N_HEADS),\n",
    "            n_layers=np.array(N_LAYERS),\n",
    "            source_audio_npz=os.path.basename(p),\n",
    "        )\n",
    "        print(f\"[ok]   {os.path.basename(p)} -> h_audio {h_np.shape}\" + (f\", pooled {poolnp.shape}\" if SAVE_POOLED else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca272ba",
   "metadata": {},
   "source": [
    "# Self Attention to get the h_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b617afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn\n",
    "\n",
    "class GraphlessGraphEncoder(nn.Module):\n",
    "    def __init__(self, d_in, prefer_heads=(8, 6, 4, 3, 2, 1), d_model=None, n_heads=None,\n",
    "                 n_layers=2, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Decide d_model and n_heads automatically if not provided\n",
    "        if d_model is None and n_heads is None:\n",
    "            # try to keep d_model=d_in and pick a head count that divides it\n",
    "            for h in prefer_heads:\n",
    "                if d_in % h == 0:\n",
    "                    d_model, n_heads = d_in, h\n",
    "                    break\n",
    "            else:\n",
    "                # no divisor found -> project to next multiple of top preference (e.g., 8)\n",
    "                h = prefer_heads[0]\n",
    "                d_model = int(math.ceil(d_in / h) * h)\n",
    "                n_heads = h\n",
    "        else:\n",
    "            d_model = d_in if d_model is None else int(d_model)\n",
    "            n_heads = 8 if n_heads is None else int(n_heads)\n",
    "            if d_model % n_heads != 0:\n",
    "                raise ValueError(f\"d_model={d_model} must be divisible by n_heads={n_heads}\")\n",
    "\n",
    "        self.d_in    = d_in\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.in_proj = nn.Linear(d_in, d_model) if d_in != d_model else nn.Identity()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.out_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, spec_tokens, mask=None):\n",
    "        if spec_tokens.dim() == 2:  # [L,D] -> [1,L,D]\n",
    "            spec_tokens = spec_tokens.unsqueeze(0)\n",
    "        B, L, _ = spec_tokens.shape\n",
    "        if mask is None:\n",
    "            mask = torch.ones((B, L), dtype=torch.bool, device=spec_tokens.device)\n",
    "        elif mask.dim() == 1:\n",
    "            mask = mask.unsqueeze(0).to(spec_tokens.device)\n",
    "        else:\n",
    "            mask = mask.to(spec_tokens.device)\n",
    "\n",
    "        x = self.in_proj(spec_tokens)\n",
    "        kpm = ~mask.bool()\n",
    "        x = self.encoder(x, src_key_padding_mask=kpm)\n",
    "        x = self.out_norm(x)\n",
    "        x = x.masked_fill(kpm.unsqueeze(-1), 0.0)\n",
    "\n",
    "        m = mask.unsqueeze(-1).float()\n",
    "        h_graph = (x * m).sum(1) / m.sum(1).clamp(min=1e-6)\n",
    "        return x, h_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7104c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] encoder: d_in=512, d_model=512, heads=8\n",
      "[ok] HC_ReadText_Spectrogram_CLIP_features.npy -> torch.Size([1, 512])\n",
      "[ok] HC_Spontaneous_Spectrogram_CLIP_features.npy -> torch.Size([1, 512])\n",
      "[ok] PD_ReadText_Spectrogram_CLIP_features.npy -> torch.Size([1, 512])\n",
      "[ok] PD_Spontaneous_Spectrogram_CLIP_features.npy -> torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roshidatdnslab/anaconda3/envs/sheedah/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, torch\n",
    "\n",
    "HC_ReadText_Spectrogram_CLIP_NPY_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/CLIP_Spectrogram_embeddings\"\n",
    "HC_ReadText_Spectrogram_OUT_DIR      = os.path.join(HC_ReadText_Spectrogram_CLIP_NPY_DIR, \"..\", \"h_graph_selfattn\")\n",
    "os.makedirs(HC_ReadText_Spectrogram_OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = None\n",
    "cached_d_in = None\n",
    "\n",
    "for p in sorted(glob.glob(os.path.join(HC_ReadText_Spectrogram_CLIP_NPY_DIR, \"*.npy\"))):\n",
    "    base = os.path.splitext(os.path.basename(p))[0]\n",
    "    out_path = os.path.join(HC_ReadText_Spectrogram_OUT_DIR, f\"{base}_h_graph_selfattn.npz\")\n",
    "    if os.path.exists(out_path):\n",
    "        print(\"[skip]\", os.path.basename(out_path)); continue\n",
    "\n",
    "    feats = np.load(p)\n",
    "    if feats.ndim == 1: feats = feats[None, :]\n",
    "    if feats.ndim == 2: feats = feats[None, :, :]\n",
    "    mask = np.ones(feats.shape[:-1], dtype=bool)\n",
    "\n",
    "    x = torch.from_numpy(feats).float().to(device)    # [B, L, D_in]\n",
    "    m = torch.from_numpy(mask).bool().to(device)\n",
    "    d_in = x.shape[-1]\n",
    "    if d_in != cached_d_in:\n",
    "        cached_d_in = d_in\n",
    "        encoder = GraphlessGraphEncoder(d_in=d_in, prefer_heads=(8,6,4,3,2,1), n_layers=2).to(device).eval()\n",
    "        print(f\"[init] encoder: d_in={d_in}, d_model={encoder.d_model}, heads={encoder.n_heads}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h_nodes, h_vec = encoder(x, m)                # [B, L, D], [B, D]\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        h_graph_nodes=h_nodes.cpu().numpy().astype(np.float32),\n",
    "        attention_mask=mask.astype(bool),\n",
    "        h_graph=h_vec.cpu().numpy().astype(np.float32),\n",
    "        d_model=np.array(encoder.d_model),\n",
    "        source_clip_npy=os.path.basename(p),\n",
    "    )\n",
    "    print(\"[ok]\", os.path.basename(p), \"->\", h_vec.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheedah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
